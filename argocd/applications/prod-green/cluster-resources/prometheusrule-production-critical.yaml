apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: production-critical-alerts
  namespace: monitoring
  labels:
    release: kube-prometheus-stack
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  groups:
    # ============================================
    # CATEGORY 1: APPLICATION AVAILABILITY
    # ============================================
    - name: application_availability
      interval: 30s
      rules:
        - alert: PodCrashLooping
          expr: |
            rate(kube_pod_container_status_restarts_total[15m]) > 0
            and
            kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"} == 1
          for: 5m
          labels:
            severity: critical
            category: availability
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
            description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been restarting repeatedly for more than 5 minutes. Container: {{ $labels.container }}"
        
        - alert: DeploymentUnavailable
          expr: |
            kube_deployment_status_replicas_unavailable > 0
          for: 5m
          labels:
            severity: critical
            category: availability
          annotations:
            summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has unavailable replicas"
            description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} has {{ $value }} unavailable replica(s) for more than 5 minutes"
        
        - alert: StatefulSetUnavailable
          expr: |
            (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas) < 1
          for: 5m
          labels:
            severity: critical
            category: availability
          annotations:
            summary: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} is not fully ready"
            description: "StatefulSet {{ $labels.statefulset }} in namespace {{ $labels.namespace }} has {{ $value | humanizePercentage }} ready replicas"
        
        - alert: PodNotReady
          expr: |
            sum by (namespace, pod) (kube_pod_status_condition{condition="Ready",status="false"}) > 0
          for: 10m
          labels:
            severity: critical
            category: availability
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"
            description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been not ready for more than 10 minutes"
        
        - alert: ServiceEndpointsDown
          expr: |
            kube_endpoint_address_available == 0
            and
            kube_endpoint_labels_service_name != ""
          for: 5m
          labels:
            severity: critical
            category: availability
          annotations:
            summary: "Service {{ $labels.namespace }}/{{ $labels.service }} has no available endpoints"
            description: "Service {{ $labels.service }} in namespace {{ $labels.namespace }} has no ready endpoints for more than 5 minutes"

    # ============================================
    # CATEGORY 2: APPLICATION ERRORS
    # ============================================
    - name: application_errors
      interval: 30s
      rules:
        - alert: HighErrorRate
          expr: |
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (namespace, service) > 0.1
          for: 5m
          labels:
            severity: critical
            category: errors
          annotations:
            summary: "High 5xx error rate in {{ $labels.namespace }}/{{ $labels.service }}"
            description: "Service {{ $labels.service }} in namespace {{ $labels.namespace }} has error rate of {{ $value | humanize }} req/s for more than 5 minutes"
        
        - alert: CriticalErrorRate
          expr: |
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (namespace, service) > 1
          for: 2m
          labels:
            severity: critical
            category: errors
          annotations:
            summary: "CRITICAL: Very high error rate in {{ $labels.namespace }}/{{ $labels.service }}"
            description: "Service {{ $labels.service }} has critical error rate of {{ $value | humanize }} req/s. Immediate attention required!"
        
        - alert: HighClientErrorRate
          expr: |
            sum(rate(http_requests_total{status=~"4.."}[5m])) by (namespace, service) > 5
          for: 10m
          labels:
            severity: warning
            category: errors
          annotations:
            summary: "High 4xx client error rate in {{ $labels.namespace }}/{{ $labels.service }}"
            description: "Service {{ $labels.service }} has high client error rate of {{ $value | humanize }} req/s for 10 minutes"

    # ============================================
    # CATEGORY 3: APPLICATION PERFORMANCE
    # ============================================
    - name: application_performance
      interval: 30s
      rules:
        - alert: HighLatency
          expr: |
            histogram_quantile(0.95, 
              sum(rate(http_request_duration_seconds_bucket{handler!="/metrics"}[5m])) by (le, namespace, service)
            ) > 1
          for: 10m
          labels:
            severity: warning
            category: performance
          annotations:
            summary: "High p95 latency in {{ $labels.namespace }}/{{ $labels.service }}"
            description: "95th percentile latency is {{ $value | humanize }}s (threshold: 1s) for 10 minutes"
        
        - alert: CriticalLatency
          expr: |
            histogram_quantile(0.99, 
              sum(rate(http_request_duration_seconds_bucket{handler!="/metrics"}[5m])) by (le, namespace, service)
            ) > 3
          for: 5m
          labels:
            severity: critical
            category: performance
          annotations:
            summary: "CRITICAL: Very high p99 latency in {{ $labels.namespace }}/{{ $labels.service }}"
            description: "99th percentile latency is {{ $value | humanize }}s (threshold: 3s) for 5 minutes"

    # ============================================
    # CATEGORY 4: INFRASTRUCTURE HEALTH
    # ============================================
    - name: infrastructure_health
      interval: 30s
      rules:
        - alert: NodeNotReady
          expr: |
            kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 5m
          labels:
            severity: critical
            category: infrastructure
          annotations:
            summary: "Node {{ $labels.node }} is not ready"
            description: "Node {{ $labels.node }} has been not ready for more than 5 minutes"
        
        - alert: NodeMemoryPressure
          expr: |
            kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
          for: 5m
          labels:
            severity: warning
            category: infrastructure
          annotations:
            summary: "Node {{ $labels.node }} is under memory pressure"
            description: "Node {{ $labels.node }} has memory pressure condition for more than 5 minutes"
        
        - alert: NodeDiskPressure
          expr: |
            kube_node_status_condition{condition="DiskPressure",status="true"} == 1
          for: 2m
          labels:
            severity: critical
            category: infrastructure
          annotations:
            summary: "CRITICAL: Node {{ $labels.node }} is under disk pressure"
            description: "Node {{ $labels.node }} has disk pressure condition. Pods may be evicted!"
        
        - alert: NodePIDPressure
          expr: |
            kube_node_status_condition{condition="PIDPressure",status="true"} == 1
          for: 5m
          labels:
            severity: warning
            category: infrastructure
          annotations:
            summary: "Node {{ $labels.node }} is under PID pressure"
            description: "Node {{ $labels.node }} has PID pressure condition for more than 5 minutes"

    # ============================================
    # CATEGORY 5: RESOURCE EXHAUSTION
    # ============================================
    - name: resource_exhaustion
      interval: 30s
      rules:
        - alert: PodCPUThrottling
          expr: |
            sum(rate(container_cpu_cfs_throttled_seconds_total[5m])) by (namespace, pod) > 0.1
          for: 10m
          labels:
            severity: warning
            category: resources
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is CPU throttled"
            description: "Pod {{ $labels.pod }} is being CPU throttled. Consider increasing CPU limits"
        
        - alert: HighMemoryUsage
          expr: |
            (container_memory_working_set_bytes / container_spec_memory_limit_bytes) > 0.9
            and
            container_spec_memory_limit_bytes > 0
          for: 10m
          labels:
            severity: warning
            category: resources
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is using >90% of memory limit"
            description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of its memory limit for 10 minutes"
        
        - alert: HighCPUUsage
          expr: |
            (rate(container_cpu_usage_seconds_total[5m]) / container_spec_cpu_quota) > 0.9
            and
            container_spec_cpu_quota > 0
          for: 10m
          labels:
            severity: warning
            category: resources
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is using >90% of CPU limit"
            description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of its CPU limit for 10 minutes"
        
        - alert: DiskSpaceRunningOut
          expr: |
            (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.1
          for: 5m
          labels:
            severity: critical
            category: resources
          annotations:
            summary: "CRITICAL: Node {{ $labels.instance }} disk space is running out"
            description: "Node {{ $labels.instance }} has less than 10% disk space available ({{ $value | humanizePercentage }} remaining)"

    # ============================================
    # CATEGORY 6: DATA LAYER
    # ============================================
    - name: data_layer
      interval: 30s
      rules:
        - alert: PostgresConnectionPoolExhausted
          expr: |
            sum(pg_stat_activity_count{datname="darey_score"}) > 50
          for: 5m
          labels:
            severity: critical
            category: database
          annotations:
            summary: "CRITICAL: PostgreSQL connection pool nearly exhausted"
            description: "PostgreSQL has {{ $value }} active connections (limit: 50). New connections may be rejected"
        
        - alert: PostgresHighConnections
          expr: |
            sum(pg_stat_activity_count{datname="darey_score"}) > 40
          for: 10m
          labels:
            severity: warning
            category: database
          annotations:
            summary: "PostgreSQL connection count is high"
            description: "PostgreSQL has {{ $value }} active connections (80% of limit)"
        
        - alert: RedisQueueBacklog
          expr: |
            max(redis_key_size{key="rq:queue:score_events"}) > 500
          for: 10m
          labels:
            severity: warning
            category: redis
          annotations:
            summary: "Redis queue backlog is growing"
            description: "Redis queue 'score_events' has {{ $value }} pending items for 10 minutes"
        
        - alert: RedisQueueBacklogCritical
          expr: |
            max(redis_key_size{key="rq:queue:score_events"}) > 2000
          for: 5m
          labels:
            severity: critical
            category: redis
          annotations:
            summary: "CRITICAL: Redis queue backlog is very high"
            description: "Redis queue 'score_events' has {{ $value }} pending items. Processing may be delayed significantly"
        
        - alert: PVCPending
          expr: |
            kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
          for: 10m
          labels:
            severity: critical
            category: storage
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending"
            description: "PersistentVolumeClaim {{ $labels.persistentvolumeclaim }} has been pending for more than 10 minutes"

    # ============================================
    # CATEGORY 7: NETWORK & INGRESS
    # ============================================
    - name: network_ingress
      interval: 30s
      rules:
        - alert: IngressHighLatency
          expr: |
            histogram_quantile(0.95, 
              sum(rate(nginx_ingress_controller_request_duration_seconds_bucket[5m])) by (le, ingress)
            ) > 1
          for: 10m
          labels:
            severity: warning
            category: ingress
          annotations:
            summary: "High latency on ingress {{ $labels.ingress }}"
            description: "95th percentile latency is {{ $value | humanize }}s on ingress {{ $labels.ingress }}"
        
        - alert: IngressHighErrorRate
          expr: |
            sum(rate(nginx_ingress_controller_requests{status=~"5.."}[5m])) by (ingress) > 0.1
          for: 5m
          labels:
            severity: critical
            category: ingress
          annotations:
            summary: "High 5xx error rate on ingress {{ $labels.ingress }}"
            description: "Ingress {{ $labels.ingress }} has error rate of {{ $value | humanize }} req/s for 5 minutes"

    # ============================================
    # CATEGORY 8: SECURITY
    # ============================================
    - name: security
      interval: 1h
      rules:
        - alert: CertificateExpiringSoon
          expr: |
            (certmanager_certificate_expiration_timestamp_seconds - time()) / 86400 < 30
          for: 1h
          labels:
            severity: warning
            category: security
          annotations:
            summary: "Certificate {{ $labels.name }} is expiring soon"
            description: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} will expire in {{ $value | humanize }} days"
        
        - alert: CertificateExpiringVerySoon
          expr: |
            (certmanager_certificate_expiration_timestamp_seconds - time()) / 86400 < 7
          for: 1h
          labels:
            severity: critical
            category: security
          annotations:
            summary: "CRITICAL: Certificate {{ $labels.name }} is expiring very soon"
            description: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} will expire in {{ $value | humanize }} days. Renew immediately!"

    # ============================================
    # CATEGORY 9: SERVICE-SPECIFIC ALERTS
    # ============================================
    - name: service_specific
      interval: 30s
      rules:
        - alert: DareyScoreAPIUnavailable
          expr: |
            sum(up{job="dareyscore-api"}) == 0
          for: 2m
          labels:
            severity: critical
            category: availability
            service: dareyscore-api
          annotations:
            summary: "CRITICAL: DareyScore API is unavailable"
            description: "DareyScore API service is down. All API endpoints are unreachable"
        
        - alert: LabControllerUnavailable
          expr: |
            kube_deployment_status_replicas_unavailable{namespace="lab-controller"} > 0
          for: 5m
          labels:
            severity: critical
            category: availability
            service: lab-controller
          annotations:
            summary: "CRITICAL: Lab Controller is unavailable"
            description: "Lab Controller service has {{ $value }} unavailable replica(s). Lab creation may be affected"
        
        - alert: LiveclassesUnavailable
          expr: |
            kube_deployment_status_replicas_unavailable{namespace="liveclasses"} > 0
          for: 5m
          labels:
            severity: critical
            category: availability
            service: liveclasses
          annotations:
            summary: "CRITICAL: Liveclasses service is unavailable"
            description: "Liveclasses deployment has {{ $value }} unavailable replica(s). Video conferencing may be affected"

